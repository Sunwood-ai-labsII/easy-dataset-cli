# easy_dataset_cli/main.py
"""CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""

from pathlib import Path
from typing_extensions import Annotated
import typer
from rich.console import Console
from rich.progress import Progress
from rich.text import Text
from rich.panel import Panel
from rich.columns import Columns
from rich.table import Table
from dotenv import load_dotenv
try:
    from art import tprint, text2art
    ART_AVAILABLE = True
except ImportError:
    ART_AVAILABLE = False

from .core import (
    split_text,
    parse_ga_file,
    generate_qa_for_chunk_with_ga,
    generate_qa_for_chunk_with_ga_and_fulltext,
    generate_qa_for_chunk_with_ga_and_thinking,
    convert_to_xml_by_genre,
    generate_ga_definitions,
    parse_ga_definitions_from_xml,
    save_ga_definitions_by_genre,
    create_output_directories,
    sanitize_filename,
    convert_all_xml_to_alpaca,
    upload_to_huggingface,
    create_dataset_card
)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

app = typer.Typer(
    help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ãŠã—ã‚ƒã‚ŒãªCLIãƒ„ãƒ¼ãƒ«ã€‚",
    context_settings={"help_option_names": ["-h", "--help"]}
)
console = Console()

def print_logo():
    """ãŠã—ã‚ƒã‚Œãªãƒ­ã‚´ã‚’è¡¨ç¤º"""
    if ART_AVAILABLE:
        console.print("\n")
        # ã‚·ãƒ³ãƒ—ãƒ«ã§èª­ã¿ã‚„ã™ã„ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
        try:
            logo_text = text2art("Easy Dataset CLI", font="colossal")
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆ
            logo_text = text2art("Easy Dataset CLI")
        
        # å„è¡Œã‚’ä¸­å¤®æƒãˆã«èª¿æ•´
        lines = logo_text.strip().split('\n')
        max_width = max(len(line.rstrip()) for line in lines) if lines else 0
        
        # ãƒ‘ãƒãƒ«å†…ã§ä¸­å¤®æƒãˆã™ã‚‹ãŸã‚ã€Textã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨
        logo_panel = Panel(
            Text(logo_text.strip(), style="bold cyan", justify="center"),
            title="[bold green]ğŸš€ Easy Dataset CLI[/bold green]",
            subtitle="[italic]Powered by AI[/italic]",
            border_style="bright_blue",
            padding=(1, 2),
            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
        )
        console.print(logo_panel)
    else:
        header = Panel(
            Text("ğŸš€ Easy Dataset CLI\nãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Q&Aãƒšã‚¢ã‚’è‡ªå‹•ç”Ÿæˆ", style="bold cyan", justify="center"),
            border_style="bright_blue",
            padding=(1, 2),
            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
        )
        console.print(header)

def print_success_summary(message: str, details: list = None):
    """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
    panel = Panel(
        f"[bold green]âœ¨ {message}[/bold green]",
        border_style="green",
        padding=(1, 2)
    )
    console.print(panel)
    
    if details:
        table = Table(show_header=False, box=None)
        table.add_column("Item", style="cyan")
        for detail in details:
            table.add_row(f"  â€¢ {detail}")
        console.print(table)

def print_error_panel(error_msg: str):
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
    panel = Panel(
        f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼[/bold red]\n{error_msg}",
        border_style="red",
        padding=(1, 2)
    )
    console.print(panel)


@app.command()
def create_ga(
    file_path: Annotated[Path, typer.Argument(
        exists=True, dir_okay=False, readable=True,
        help="GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€‚"
    )],
    output_dir: Annotated[Path, typer.Option(
        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
        help="ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚"
    )],
    model: Annotated[str, typer.Option(
        "--model", "-m",
        help="GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
    )] = "openrouter/openai/gpt-oss-120b",
    num_ga_pairs: Annotated[int, typer.Option(
        "--num-ga-pairs", "-g",
        help="ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
    )] = 5,
):
    """å…ƒã®æ–‡ç« ã‚’åˆ†æã—ã€GAãƒšã‚¢å®šç¾©ã‚’XMLå½¢å¼ã§ç”Ÿæˆã—ã€Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
    print_logo()
    
    info_table = Table(show_header=False, box=None)
    info_table.add_column("Key", style="bold cyan")
    info_table.add_column("Value", style="white")
    info_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
    info_table.add_row("ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(output_dir))
    info_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
    info_table.add_row("ğŸ”¢ GAãƒšã‚¢æ•°", str(num_ga_pairs))
    
    console.print(Panel(info_table, title="[bold blue]ğŸš€ GAãƒšã‚¢ç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))

    try:
        text = file_path.read_text(encoding="utf-8")
        console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]\n")

        with console.status("[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­...[/bold green]"):
            xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
        dirs = create_output_directories(output_dir)
        console.print(f"\n[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")
        
        # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        raw_file_path = dirs["logs"] / "raw.md"
        raw_file_path.write_text(xml_content, encoding="utf-8")
        console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path.name}[/cyan]")

        with console.status("[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­...[/bold green]"):
            # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æ
            ga_pairs = parse_ga_definitions_from_xml(xml_content)
        
        if not ga_pairs:
            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\nç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            console.print(Panel(xml_content, title="ç”Ÿæˆã•ã‚ŒãŸXML", border_style="yellow"))
            raise typer.Exit(code=1)

        # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
        xml_file_path = dirs["ga"] / "ga_definitions.xml"
        # XMLã‚¿ã‚°éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
        xml_start = xml_content.find("<GADefinitions>")
        xml_end = xml_content.rfind("</GADefinitions>")
        if xml_start != -1 and xml_end != -1:
            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
            xml_file_path.write_text(clean_xml, encoding="utf-8")
            console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path.name}[/cyan]")

        # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        save_ga_definitions_by_genre(ga_pairs, dirs["ga"])

        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
        details = [
            f"{len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆ",
            f"ä¿å­˜å…ˆ: {dirs['ga']}",
            "XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"
        ]
        print_success_summary("GAãƒšã‚¢ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
        
        next_steps_panel = Panel(
            "ğŸ” ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
            "âœï¸ å¿…è¦ã«å¿œã˜ã¦ç·¨é›†\n"
            "ğŸš€ `generate` ã‚³ãƒãƒ³ãƒ‰ã§Q&Aç”Ÿæˆã¸",
            title="[bold yellow]ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—[/bold yellow]",
            border_style="yellow"
        )
        console.print(next_steps_panel)

    except Exception as e:
        print_error_panel(f"GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e}")
        raise typer.Exit(code=1)


@app.command()
def generate(
    file_path: Annotated[Path, typer.Argument(
        exists=True, dir_okay=False, readable=True,
        help="å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚"
    )],
    ga_file: Annotated[Path, typer.Option(
        "--ga-file", "-g", exists=True, dir_okay=False, readable=True,
        help="Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLã¾ãŸã¯Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ga_definitions.xmlã‚’æ¨å¥¨ã€‚"
    )],
    output_dir: Annotated[Path, typer.Option(
        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
        help="ç”Ÿæˆã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚"
    )] = None,
    model: Annotated[str, typer.Option(
        "--model", "-m",
        help="Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
    )] = "openrouter/openai/gpt-oss-120b",
    chunk_size: Annotated[int, typer.Option(
        help="ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚ºã€‚"
    )] = 2000,
    chunk_overlap: Annotated[int, typer.Option(
        help="ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚"
    )] = 200,
    num_qa_pairs: Annotated[int, typer.Option(
        "--num-qa-pairs", "-q",
        help="å„ãƒãƒ£ãƒ³ã‚¯ãƒ»GAãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
    )] = 10,
    use_fulltext: Annotated[bool, typer.Option(
        "--use-fulltext", "-f",
        help="å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
    )] = False,
    use_thinking: Annotated[bool, typer.Option(
        "--use-thinking", "-T",
        help="å„Q&Aãƒšã‚¢ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã—ã¾ã™ã€‚ã‚ˆã‚Šæ·±ã„ç†è§£ã¨èª¬æ˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
    )] = False,
    append_mode: Annotated[bool, typer.Option(
        "--append", "-A",
        help="æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„Q&Aã‚’è¿½åŠ ã—ã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä¸Šæ›¸ãã—ã¾ã™ã€‚"
    )] = False,
    export_alpaca: Annotated[bool, typer.Option(
        "--export-alpaca", "-a",
        help="ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"
    )] = False,
    upload_hf: Annotated[bool, typer.Option(
        "--upload-hf", "-u",
        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
    )] = False,
    hf_repo_name: Annotated[str, typer.Option(
        "--hf-repo-name", "-r",
        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
    )] = "",
    hf_token: Annotated[str, typer.Option(
        "--hf-token", "-t",
        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
    )] = "",
    hf_private: Annotated[bool, typer.Option(
        "--hf-private",
        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
    )] = False,
):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
    
    --use-fulltextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†æ™‚ã«å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ãŸã ã—ã€å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
    """
    print_logo()
    
    try:
        # è¨­å®šæƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
        settings_table = Table(show_header=False, box=None)
        settings_table.add_column("é …ç›®", style="bold cyan")
        settings_table.add_column("å€¤", style="white")
        settings_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
        settings_table.add_row("ğŸ“Š GAå®šç¾©", str(ga_file))
        settings_table.add_row("ğŸ“ å‡ºåŠ›å…ˆ", str(output_dir) if output_dir else "ã‚³ãƒ³ã‚½ãƒ¼ãƒ«")
        settings_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
        settings_table.add_row("ğŸ”¢ Q&Aæ•°/ãƒãƒ£ãƒ³ã‚¯", str(num_qa_pairs))
        
        mode_options = []
        if use_fulltext: mode_options.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
        if use_thinking: mode_options.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼")
        if append_mode: mode_options.append("â• è¿½åŠ ãƒ¢ãƒ¼ãƒ‰")
        if export_alpaca: mode_options.append("ğŸ¤™ Alpacaå½¢å¼")
        if upload_hf: mode_options.append("ğŸ¤— HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        if mode_options:
            settings_table.add_row("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ", ".join(mode_options))
        
        console.print(Panel(settings_table, title="[bold blue]ğŸš€ Q&Aç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
        text = file_path.read_text(encoding="utf-8")
        console.print(f"\n[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]")

        with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
            ga_pairs = parse_ga_file(ga_file)

        if not ga_pairs:
            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            raise typer.Exit(code=1)

        console.print(f"\n[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")

        with console.status("âœ‚ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­..."):
            chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        console.print(f"[green]âœ“[/green] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")

        all_qa_pairs_with_ga = []
        total_tasks = len(chunks) * len(ga_pairs)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯æ§‹é€ ã‚’ä½œæˆ
        dirs = None
        if output_dir:
            dirs = create_output_directories(output_dir)
            console.print(f"[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")

        # ãƒ¢ãƒ¼ãƒ‰è­¦å‘Šã‚’è¡¨ç¤º
        warnings = []
        if use_fulltext:
            warnings.append(f"ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ ({len(text):,} æ–‡å­—)")
        if use_thinking:
            warnings.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰")
        
        if warnings:
            warning_panel = Panel(
                "\n".join(warnings) + "\n\nâš ï¸ å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                title="[bold yellow]âš ï¸ ãƒ¢ãƒ¼ãƒ‰è­¦å‘Š[/bold yellow]",
                border_style="yellow"
            )
            console.print(warning_panel)

        with Progress(console=console) as progress:
            task = progress.add_task("[green]Q&Aãƒšã‚¢ã‚’ç”Ÿæˆä¸­...", total=total_tasks)

            for chunk in chunks:
                for ga_pair in ga_pairs:
                    if use_thinking:
                        qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
                            chunk=chunk,
                            full_text=text if use_fulltext else "",
                            model=model,
                            ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )
                    elif use_fulltext:
                        qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
                            chunk=chunk,
                            full_text=text,
                            model=model,
                            ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )
                    else:
                        qa_pairs = generate_qa_for_chunk_with_ga(
                            chunk, model=model, ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )

                    for pair in qa_pairs:
                        qa_entry = {
                            "genre": ga_pair['genre']['title'],
                            "audience": ga_pair['audience']['title'],
                            "question": pair['question'],
                            "answer": pair['answer'],  # <think>...</think>å›ç­”...å½¢å¼ãŒãã®ã¾ã¾å…¥ã‚‹
                        }
                        all_qa_pairs_with_ga.append(qa_entry)

                    progress.update(
                        task, advance=1,
                        description=f"Genre: {ga_pair['genre']['title']}"
                    )

        generation_summary = Panel(
            f"âœ¨ [bold green]{len(all_qa_pairs_with_ga)}[/bold green] å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆå®Œäº†ï¼",
            title="[bold green]âœ… ç”Ÿæˆçµæœ[/bold green]",
            border_style="green"
        )
        console.print(generation_summary)

        xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"] if dirs else None, append_mode)

        if dirs:
            with console.status(f"ğŸ’¾ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {dirs['qa']} ã«ä¿å­˜ä¸­..."):
                saved_files = []
                for genre, xml_content in xml_outputs_by_genre.items():
                    safe_genre_name = sanitize_filename(genre)
                    output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
                    output_file_path.write_text(xml_content, encoding="utf-8")
                    saved_files.append(output_file_path.name)
            
            files_table = Table(show_header=False, box=None)
            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
            for file_name in saved_files:
                files_table.add_row(f"âœ“ {file_name}")
            
                console.print(Panel(files_table, title="[bold green]ğŸ’¾ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
            
            # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if export_alpaca:
                console.print("\n[bold blue]Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...[/bold blue]")
                alpaca_file = dirs["base"] / "dataset_alpaca.json"
                alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                readme_file = dirs["base"] / "README.md"
                create_dataset_card(alpaca_data, readme_file, "Generated QA Dataset")
                
                # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                if upload_hf:
                    if not hf_repo_name:
                        console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
                        console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
                    else:
                        console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
                        success = upload_to_huggingface(
                            dataset_data=alpaca_data,
                            repo_name=hf_repo_name,
                            hf_token=hf_token if hf_token else None,
                            private=hf_private,
                            commit_message=f"Upload QA dataset with {len(alpaca_data)} entries",
                            readme_file=readme_file
                        )
                        if not success:
                            console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
        else:
            console.print("\n--- ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ (Genreåˆ¥XML) ---")
            for genre, xml_content in xml_outputs_by_genre.items():
                console.print(f"\n[bold yellow]## Genre: {genre} ##[/bold yellow]")
                console.print(xml_content, overflow="fold")
    
    except Exception as e:
        import traceback
        error_details = f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}\n\nãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}"
        print_error_panel(error_details)
        raise typer.Exit(code=1)


@app.command()
def convert_to_alpaca(
    qa_dir: Annotated[Path, typer.Argument(
        exists=True, dir_okay=True, readable=True,
        help="XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹qaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
    )],
    output_file: Annotated[Path, typer.Option(
        "--output-file", "-o", file_okay=True, dir_okay=False,
        help="å‡ºåŠ›ã™ã‚‹Alpacaå½¢å¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚"
    )] = None,
    upload_hf: Annotated[bool, typer.Option(
        "--upload-hf", "-u",
        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
    )] = False,
    hf_repo_name: Annotated[str, typer.Option(
        "--hf-repo-name", "-r",
        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
    )] = "",
    hf_token: Annotated[str, typer.Option(
        "--hf-token", "-t",
        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
    )] = "",
    hf_private: Annotated[bool, typer.Option(
        "--hf-private",
        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
    )] = False,
):
    """æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã®JSONã«å¤‰æ›ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
    
    print_logo()
    
    conversion_table = Table(show_header=False, box=None)
    conversion_table.add_column("é …ç›®", style="bold cyan")
    conversion_table.add_column("å€¤", style="white")
    conversion_table.add_row("ğŸ“ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(qa_dir))
    conversion_table.add_row("ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(output_file) if output_file else "è‡ªå‹•")
    if upload_hf:
        conversion_table.add_row("ğŸ¤— HFãƒªãƒã‚¸ãƒˆãƒª", hf_repo_name or "æœªæŒ‡å®š")
    
    console.print(Panel(conversion_table, title="[bold blue]ğŸ”„ Alpacaå½¢å¼å¤‰æ›[/bold blue]", border_style="blue"))
    
    try:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
        if output_file is None:
            output_file = qa_dir.parent / "dataset_alpaca.json"
        
        with console.status(f"ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã«å¤‰æ›ä¸­..."):
            alpaca_data = convert_all_xml_to_alpaca(qa_dir, output_file)
        
        if not alpaca_data:
            print_error_panel("å¤‰æ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            raise typer.Exit(code=1)
        
        with console.status("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
            readme_file = output_file.parent / "README.md"
            create_dataset_card(alpaca_data, readme_file, "Converted QA Dataset")
        
        # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if upload_hf:
            if not hf_repo_name:
                print_error_panel("--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼\nä¾‹: --hf-repo-name username/my-qa-dataset")
                raise typer.Exit(code=1)
            
            with console.status(f"ğŸ¤— Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                success = upload_to_huggingface(
                    dataset_data=alpaca_data,
                    repo_name=hf_repo_name,
                    hf_token=hf_token if hf_token else None,
                    private=hf_private,
                    commit_message=f"Upload converted QA dataset with {len(alpaca_data)} entries",
                    readme_file=readme_file
                )
            
            if not success:
                print_error_panel("Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                raise typer.Exit(code=1)
        
        details = [
            f"{len(alpaca_data)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å¤‰æ›",
            f"å‡ºåŠ›å…ˆ: {output_file}",
            f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰: {readme_file}"
        ]
        if upload_hf and hf_repo_name:
            details.append(f"Hugging Face: {hf_repo_name}")
        
        print_success_summary("Alpacaå½¢å¼ã¸ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
        
    except Exception as e:
        print_error_panel(f"å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise typer.Exit(code=1)


@app.command()
def aggregate_logs(
    output_dir: Annotated[Path, typer.Argument(
        exists=True, dir_okay=True, readable=True,
        help="logsãƒ•ã‚©ãƒ«ãƒ€ãŒå«ã¾ã‚Œã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
    )]
):
    """logsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ã—ã¦qaãƒ•ã‚©ãƒ«ãƒ€ã®XMLã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
    print_logo()
    
    try:
        logs_dir = output_dir / "logs"
        qa_dir = output_dir / "qa"
        
        if not logs_dir.exists():
            print_error_panel(f"logsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {logs_dir}")
            raise typer.Exit(code=1)
        
        aggregation_table = Table(show_header=False, box=None)
        aggregation_table.add_column("é …ç›®", style="bold cyan")
        aggregation_table.add_column("ãƒ‘ã‚¹", style="white")
        aggregation_table.add_row("ğŸ“ logsãƒ•ã‚©ãƒ«ãƒ€", str(logs_dir))
        aggregation_table.add_row("ğŸ¯ å‡ºåŠ›å…ˆ", str(qa_dir))
        
        console.print(Panel(aggregation_table, title="[bold blue]ğŸ“„ ãƒ­ã‚°é›†ç´„[/bold blue]", border_style="blue"))
        
        with console.status("ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ä¸­..."):
            from easy_dataset_cli.core import aggregate_logs_xml_to_qa
            aggregate_logs_xml_to_qa(logs_dir, qa_dir)
        
        print_success_summary("ãƒ­ã‚°é›†ç´„ãŒå®Œäº†ã—ã¾ã—ãŸï¼", [f"å‡ºåŠ›å…ˆ: {qa_dir}"])
        
    except Exception as e:
        print_error_panel(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise typer.Exit(code=1)


@app.command(name="help", hidden=True)
def show_help():
    """ãƒ˜ãƒ«ãƒ—ã‚’ç¾ã—ãè¡¨ç¤º"""
    print_logo()
    console.print(app.get_help(typer.Context(app)))


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ˜ãƒ«ãƒ—æ™‚ã«ãƒ­ã‚´ã‚’è¡¨ç¤º"""
    import sys
    if len(sys.argv) == 1 or (len(sys.argv) == 2 and sys.argv[1] in ["-h", "--help"]):
        print_logo()
    app()


if __name__ == "__main__":
    main()
