# easy_dataset_cli/main.py
"""CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""

from pathlib import Path
from typing_extensions import Annotated
import typer
from rich.console import Console
from rich.progress import Progress
from rich.text import Text
from rich.panel import Panel
from rich.columns import Columns
from rich.table import Table
from dotenv import load_dotenv
try:
    from art import tprint, text2art
    ART_AVAILABLE = True
except ImportError:
    ART_AVAILABLE = False

from .core import (
    split_text,
    parse_ga_file,
    generate_qa_for_chunk_with_ga,
    generate_qa_for_chunk_with_ga_and_fulltext,
    generate_qa_for_chunk_with_ga_and_thinking,
    convert_to_xml_by_genre,
    generate_ga_definitions,
    parse_ga_definitions_from_xml,
    save_ga_definitions_by_genre,
    create_output_directories,
    sanitize_filename,
    convert_all_xml_to_alpaca,
    upload_to_huggingface,
    create_dataset_card,
    find_text_files
)

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

app = typer.Typer(
    help="ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹ãŠã—ã‚ƒã‚ŒãªCLIãƒ„ãƒ¼ãƒ«ã€‚",
    context_settings={"help_option_names": ["-h", "--help"]}
)
console = Console()

def print_logo():
    """ãŠã—ã‚ƒã‚Œãªãƒ­ã‚´ã‚’è¡¨ç¤º"""
    if ART_AVAILABLE:
        console.print("\n")
        # ã‚·ãƒ³ãƒ—ãƒ«ã§èª­ã¿ã‚„ã™ã„ãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨
        try:
            logo_text = text2art("Easy Dataset CLI", font="colossal")
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æ¨™æº–ãƒ•ã‚©ãƒ³ãƒˆ
            logo_text = text2art("Easy Dataset CLI")
        
        # å„è¡Œã‚’ä¸­å¤®æƒãˆã«èª¿æ•´
        lines = logo_text.strip().split('\n')
        max_width = max(len(line.rstrip()) for line in lines) if lines else 0
        
        # ãƒ‘ãƒãƒ«å†…ã§ä¸­å¤®æƒãˆã™ã‚‹ãŸã‚ã€Textã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨
        logo_panel = Panel(
            Text(logo_text.strip(), style="bold cyan", justify="center"),
            title="[bold green]ğŸš€ Easy Dataset CLI[/bold green]",
            subtitle="[italic]Powered by AI[/italic]",
            border_style="bright_blue",
            padding=(1, 2),
            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
        )
        console.print(logo_panel)
    else:
        header = Panel(
            Text("ğŸš€ Easy Dataset CLI\nãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Q&Aãƒšã‚¢ã‚’è‡ªå‹•ç”Ÿæˆ", style="bold cyan", justify="center"),
            border_style="bright_blue",
            padding=(1, 2),
            expand=True  # æ¨ªå¹…ä¸€æ¯ã«å±•é–‹
        )
        console.print(header)

def print_success_summary(message: str, details: list = None):
    """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
    panel = Panel(
        f"[bold green]âœ¨ {message}[/bold green]",
        border_style="green",
        padding=(1, 2)
    )
    console.print(panel)
    
    if details:
        table = Table(show_header=False, box=None)
        table.add_column("Item", style="cyan")
        for detail in details:
            table.add_row(f"  â€¢ {detail}")
        console.print(table)

def print_error_panel(error_msg: str):
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º"""
    panel = Panel(
        f"[bold red]âŒ ã‚¨ãƒ©ãƒ¼[/bold red]\n{error_msg}",
        border_style="red",
        padding=(1, 2)
    )
    console.print(panel)


@app.command()
def create_ga(
    file_path: Annotated[Path, typer.Argument(
        exists=True, readable=True,
        help="GAãƒšã‚¢ã®å®šç¾©ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ãŸå ´åˆã€å†…éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™ã€‚"
    )],
    output_dir: Annotated[Path, typer.Option(
        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
        help="ç”Ÿæˆã•ã‚ŒãŸGAãƒšã‚¢å®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚"
    )],
    model: Annotated[str, typer.Option(
        "--model", "-m",
        help="GAãƒšã‚¢å®šç¾©ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
    )] = "openrouter/openai/gpt-oss-120b",
    num_ga_pairs: Annotated[int, typer.Option(
        "--num-ga-pairs", "-g",
        help="ç”Ÿæˆã™ã‚‹GAãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
    )] = 5,
):
    """å…ƒã®æ–‡ç« ã‚’åˆ†æã—ã€GAãƒšã‚¢å®šç¾©ã‚’XMLå½¢å¼ã§ç”Ÿæˆã—ã€Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚"""
    print_logo()
    
    try:
        # ãƒ•ã‚©ãƒ«ãƒ€ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
        if file_path.is_dir():
            # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆï¼šãƒãƒƒãƒå‡¦ç†
            console.print(f"[bold blue]ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {file_path}[/bold blue]")
            text_files = find_text_files(file_path)
            
            if not text_files:
                print_error_panel(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
                raise typer.Exit(code=1)
            
            console.print(f"[green]âœ“[/green] {len(text_files)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
            
            # ãƒãƒƒãƒå‡¦ç†ç”¨ã®è¨­å®šãƒ†ãƒ¼ãƒ–ãƒ«
            batch_info_table = Table(show_header=False, box=None)
            batch_info_table.add_column("Key", style="bold cyan")
            batch_info_table.add_column("Value", style="white")
            batch_info_table.add_row("ğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€", str(file_path))
            batch_info_table.add_row("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°", str(len(text_files)))
            batch_info_table.add_row("ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(output_dir))
            batch_info_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
            batch_info_table.add_row("ğŸ”¢ GAãƒšã‚¢æ•°", str(num_ga_pairs))
            
            console.print(Panel(batch_info_table, title="[bold blue]ğŸš€ ãƒãƒƒãƒGAãƒšã‚¢ç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            files_table = Table(show_header=False, box=None)
            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
            for text_file in text_files[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                files_table.add_row(f"â€¢ {text_file.name}")
            if len(text_files) > 10:
                files_table.add_row(f"... and {len(text_files) - 10} more files")
            
            console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
            
            return _batch_create_ga_files(text_files, output_dir, model, num_ga_pairs)
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼šæ—¢å­˜ã®å‡¦ç†
            info_table = Table(show_header=False, box=None)
            info_table.add_column("Key", style="bold cyan")
            info_table.add_column("Value", style="white")
            info_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
            info_table.add_row("ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(output_dir))
            info_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
            info_table.add_row("ğŸ”¢ GAãƒšã‚¢æ•°", str(num_ga_pairs))
            
            console.print(Panel(info_table, title="[bold blue]ğŸš€ GAãƒšã‚¢ç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
            
            text = file_path.read_text(encoding="utf-8")
            console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]\n")

        with console.status("[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­...[/bold green]"):
            xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs)

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
        dirs = create_output_directories(output_dir)
        console.print(f"\n[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")
        
        # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        raw_file_path = dirs["logs"] / "raw.md"
        raw_file_path.write_text(xml_content, encoding="utf-8")
        console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path.name}[/cyan]")

        with console.status("[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­...[/bold green]"):
            # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æ
            ga_pairs = parse_ga_definitions_from_xml(xml_content)
        
        if not ga_pairs:
            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\nç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            console.print(Panel(xml_content, title="ç”Ÿæˆã•ã‚ŒãŸXML", border_style="yellow"))
            raise typer.Exit(code=1)

        # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
        xml_file_path = dirs["ga"] / "ga_definitions.xml"
        # XMLã‚¿ã‚°éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ä¿å­˜
        xml_start = xml_content.find("<GADefinitions>")
        xml_end = xml_content.rfind("</GADefinitions>")
        if xml_start != -1 and xml_end != -1:
            clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
            xml_file_path.write_text(clean_xml, encoding="utf-8")
            console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path.name}[/cyan]")

        # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        save_ga_definitions_by_genre(ga_pairs, dirs["ga"])

        # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
        details = [
            f"{len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆ",
            f"ä¿å­˜å…ˆ: {dirs['ga']}",
            "XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"
        ]
        print_success_summary("GAãƒšã‚¢ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
        
        next_steps_panel = Panel(
            "ğŸ” ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
            "âœï¸ å¿…è¦ã«å¿œã˜ã¦ç·¨é›†\n"
            "ğŸš€ `generate` ã‚³ãƒãƒ³ãƒ‰ã§Q&Aç”Ÿæˆã¸",
            title="[bold yellow]ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—[/bold yellow]",
            border_style="yellow"
        )
        console.print(next_steps_panel)

    except Exception as e:
        print_error_panel(f"GAå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{e}")
        raise typer.Exit(code=1)


@app.command()
def generate(
    file_path: Annotated[Path, typer.Argument(
        exists=True, readable=True,
        help="å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ãƒ‘ã‚¹ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ãŸå ´åˆã€å†…éƒ¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™ã€‚"
    )],
    ga_file: Annotated[Path, typer.Option(
        "--ga-file", "-g", exists=True, dir_okay=False, readable=True,
        help="Genre-Audienceãƒšã‚¢ã‚’å®šç¾©ã—ãŸXMLã¾ãŸã¯Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹ã€‚ãƒãƒƒãƒå‡¦ç†ã§å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«å…±é€šã®å®šç¾©ã‚’é©ç”¨ã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚"
    )] = None,
    ga_base_dir: Annotated[Path, typer.Option(
        "--ga-base-dir", "-gb", exists=True, file_okay=False, dir_okay=True, readable=True,
        help="GAå®šç¾©ãƒ•ã‚©ãƒ«ãƒ€ã®è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚ãƒãƒƒãƒå‡¦ç†æ™‚ã«å„å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚"
    )] = None,
    output_dir: Annotated[Path, typer.Option(
        "--output-dir", "-o", file_okay=False, dir_okay=True, writable=True,
        help="ç”Ÿæˆã•ã‚ŒãŸXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã—ã¾ã™ã€‚"
    )] = None,
    model: Annotated[str, typer.Option(
        "--model", "-m",
        help="Q&Aãƒšã‚¢ã®ç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«åã€‚"
    )] = "openrouter/openai/gpt-oss-120b",
    chunk_size: Annotated[int, typer.Option(
        help="ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§ã‚µã‚¤ã‚ºã€‚"
    )] = 2000,
    chunk_overlap: Annotated[int, typer.Option(
        help="ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚"
    )] = 200,
    num_qa_pairs: Annotated[int, typer.Option(
        "--num-qa-pairs", "-q",
        help="å„ãƒãƒ£ãƒ³ã‚¯ãƒ»GAãƒšã‚¢ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯LLMãŒé©åˆ‡ãªæ•°ã‚’æ±ºå®šã—ã¾ã™ã€‚"
    )] = 10,
    use_fulltext: Annotated[bool, typer.Option(
        "--use-fulltext", "-f",
        help="å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã¦QAç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸQAãŒç”Ÿæˆã•ã‚Œã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
    )] = False,
    use_thinking: Annotated[bool, typer.Option(
        "--use-thinking", "-T",
        help="å„Q&Aãƒšã‚¢ã«æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½åŠ ã—ã¦ç”Ÿæˆã—ã¾ã™ã€‚ã‚ˆã‚Šæ·±ã„ç†è§£ã¨èª¬æ˜ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ã¨ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚"
    )] = False,
    append_mode: Annotated[bool, typer.Option(
        "--append", "-A",
        help="æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ–°ã—ã„Q&Aã‚’è¿½åŠ ã—ã¾ã™ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä¸Šæ›¸ãã—ã¾ã™ã€‚"
    )] = False,
    export_alpaca: Annotated[bool, typer.Option(
        "--export-alpaca", "-a",
        help="ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ã‚’Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚"
    )] = False,
    upload_hf: Annotated[bool, typer.Option(
        "--upload-hf", "-u",
        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
    )] = False,
    hf_repo_name: Annotated[str, typer.Option(
        "--hf-repo-name", "-r",
        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
    )] = "",
    hf_token: Annotated[str, typer.Option(
        "--hf-token", "-t",
        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
    )] = "",
    hf_private: Annotated[bool, typer.Option(
        "--hf-private",
        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
    )] = False,
):
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨GAå®šç¾©ã‹ã‚‰Q&Aãƒšã‚¢ã‚’ç”Ÿæˆã—ã€Genreåˆ¥ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚
    
    --use-fulltextã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å„ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†æ™‚ã«å…¨æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€
    ã‚ˆã‚Šæ–‡è„ˆã‚’ç†è§£ã—ãŸé«˜å“è³ªãªQ&Aãƒšã‚¢ã‚’ç”Ÿæˆã§ãã¾ã™ã€‚ãŸã ã—ã€å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã¾ã™ã€‚
    """
    print_logo()
    
    try:
        # ãƒ•ã‚©ãƒ«ãƒ€ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚’åˆ¤å®š
        if file_path.is_dir():
            # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆï¼šãƒãƒƒãƒå‡¦ç†
            console.print(f"[bold blue]ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {file_path}[/bold blue]")
            text_files = find_text_files(file_path)
            
            if not text_files:
                print_error_panel(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {file_path}")
                raise typer.Exit(code=1)
            
            console.print(f"[green]âœ“[/green] {len(text_files)}å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
            
            # ãƒãƒƒãƒå‡¦ç†ç”¨ã®è¨­å®šãƒ†ãƒ¼ãƒ–ãƒ«
            batch_settings_table = Table(show_header=False, box=None)
            batch_settings_table.add_column("é …ç›®", style="bold cyan")
            batch_settings_table.add_column("å€¤", style="white")
            batch_settings_table.add_row("ğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€", str(file_path))
            batch_settings_table.add_row("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æ•°", str(len(text_files)))
            
            # GAå®šç¾©ã®è¡¨ç¤º
            if ga_file:
                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©", str(ga_file))
            elif ga_base_dir:
                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(ga_base_dir))
            else:
                batch_settings_table.add_row("ğŸ“Š GAå®šç¾©", "æœªæŒ‡å®š")
            
            batch_settings_table.add_row("ï¿½ å‡ºåŠ›å…ˆ", str(output_dir) if output_dir else "ã‚³ãƒ³ã‚½ãƒ¼ãƒ«")
            batch_settings_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
            batch_settings_table.add_row("ğŸ”¢ Q&Aæ•°/ãƒãƒ£ãƒ³ã‚¯", str(num_qa_pairs))
            
            mode_options = []
            if use_fulltext: mode_options.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
            if use_thinking: mode_options.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼")
            if append_mode: mode_options.append("â• è¿½åŠ ãƒ¢ãƒ¼ãƒ‰")
            if export_alpaca: mode_options.append("ğŸ¤™ Alpacaå½¢å¼")
            if upload_hf: mode_options.append("ğŸ¤— HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
            
            if mode_options:
                batch_settings_table.add_row("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ", ".join(mode_options))
            
            console.print(Panel(batch_settings_table, title="[bold blue]ğŸš€ ãƒãƒƒãƒQ&Aç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            files_table = Table(show_header=False, box=None)
            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
            for text_file in text_files[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                files_table.add_row(f"â€¢ {text_file.name}")
            if len(text_files) > 10:
                files_table.add_row(f"... and {len(text_files) - 10} more files")
            
            console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
            
            # ãƒãƒƒãƒå‡¦ç†ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not ga_file and not ga_base_dir:
                print_error_panel("ãƒãƒƒãƒå‡¦ç†ã‚’è¡Œã†ã«ã¯ã€--ga-file ã¾ãŸã¯ --ga-base-dir ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
                raise typer.Exit(code=1)
            
            if ga_file and ga_base_dir:
                print_error_panel("--ga-file ã¨ --ga-base-dir ã¯åŒæ™‚ã«ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
                raise typer.Exit(code=1)
            
            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†
            return _batch_process_files(text_files, ga_file, ga_base_dir, output_dir, model, chunk_size, chunk_overlap,
                                      num_qa_pairs, use_fulltext, use_thinking, append_mode,
                                      export_alpaca, upload_hf, hf_repo_name, hf_token, hf_private)
        else:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼šæ—¢å­˜ã®å‡¦ç†
            # è¨­å®šæƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
            settings_table = Table(show_header=False, box=None)
            settings_table.add_column("é …ç›®", style="bold cyan")
            settings_table.add_column("å€¤", style="white")
            settings_table.add_row("ğŸ“„ å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(file_path))
            settings_table.add_row("ğŸ“Š GAå®šç¾©", str(ga_file) if ga_file else "æœªæŒ‡å®š")
            settings_table.add_row("ğŸ“ å‡ºåŠ›å…ˆ", str(output_dir) if output_dir else "ã‚³ãƒ³ã‚½ãƒ¼ãƒ«")
            settings_table.add_row("ğŸ¤– ãƒ¢ãƒ‡ãƒ«", model)
            settings_table.add_row("ğŸ”¢ Q&Aæ•°/ãƒãƒ£ãƒ³ã‚¯", str(num_qa_pairs))
            
            mode_options = []
            if use_fulltext: mode_options.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ")
            if use_thinking: mode_options.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼")
            if append_mode: mode_options.append("â• è¿½åŠ ãƒ¢ãƒ¼ãƒ‰")
            if export_alpaca: mode_options.append("ğŸ¤™ Alpacaå½¢å¼")
            if upload_hf: mode_options.append("ğŸ¤— HFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
            
            if mode_options:
                settings_table.add_row("âš™ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³", ", ".join(mode_options))
            
            console.print(Panel(settings_table, title="[bold blue]ğŸš€ Q&Aç”Ÿæˆè¨­å®š[/bold blue]", border_style="blue"))
            
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not ga_file:
                print_error_panel("å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã«ã¯ --ga-file ã®æŒ‡å®šãŒå¿…é ˆã§ã™ã€‚")
                raise typer.Exit(code=1)
            
            text = file_path.read_text(encoding="utf-8")
            console.print(f"\n[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ[/dim]")

        with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
            ga_pairs = parse_ga_file(ga_file)

        if not ga_pairs:
            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            raise typer.Exit(code=1)

        console.print(f"\n[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")

        with console.status("âœ‚ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­..."):
            chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        console.print(f"[green]âœ“[/green] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")

        all_qa_pairs_with_ga = []
        total_tasks = len(chunks) * len(ga_pairs)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚‹å ´åˆã¯æ§‹é€ ã‚’ä½œæˆ
        dirs = None
        if output_dir:
            dirs = create_output_directories(output_dir)
            console.print(f"[dim]âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: ga/, logs/, qa/[/dim]")

        # ãƒ¢ãƒ¼ãƒ‰è­¦å‘Šã‚’è¡¨ç¤º
        warnings = []
        if use_fulltext:
            warnings.append(f"ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ ({len(text):,} æ–‡å­—)")
        if use_thinking:
            warnings.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰")
        
        if warnings:
            warning_panel = Panel(
                "\n".join(warnings) + "\n\nâš ï¸ å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                title="[bold yellow]âš ï¸ ãƒ¢ãƒ¼ãƒ‰è­¦å‘Š[/bold yellow]",
                border_style="yellow"
            )
            console.print(warning_panel)

        with Progress(console=console) as progress:
            task = progress.add_task("[green]Q&Aãƒšã‚¢ã‚’ç”Ÿæˆä¸­...", total=total_tasks)

            for chunk in chunks:
                for ga_pair in ga_pairs:
                    if use_thinking:
                        qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
                            chunk=chunk,
                            full_text=text if use_fulltext else "",
                            model=model,
                            ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )
                    elif use_fulltext:
                        qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
                            chunk=chunk,
                            full_text=text,
                            model=model,
                            ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )
                    else:
                        qa_pairs = generate_qa_for_chunk_with_ga(
                            chunk, model=model, ga_pair=ga_pair,
                            logs_dir=dirs["logs"] if dirs else None,
                            num_qa_pairs=num_qa_pairs
                        )

                    for pair in qa_pairs:
                        qa_entry = {
                            "genre": ga_pair['genre']['title'],
                            "audience": ga_pair['audience']['title'],
                            "question": pair['question'],
                            "answer": pair['answer'],  # <think>...</think>å›ç­”...å½¢å¼ãŒãã®ã¾ã¾å…¥ã‚‹
                        }
                        all_qa_pairs_with_ga.append(qa_entry)

                    progress.update(
                        task, advance=1,
                        description=f"Genre: {ga_pair['genre']['title']}"
                    )

        generation_summary = Panel(
            f"âœ¨ [bold green]{len(all_qa_pairs_with_ga)}[/bold green] å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆå®Œäº†ï¼",
            title="[bold green]âœ… ç”Ÿæˆçµæœ[/bold green]",
            border_style="green"
        )
        console.print(generation_summary)

        xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"] if dirs else None, append_mode)

        if dirs:
            with console.status(f"ğŸ’¾ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ {dirs['qa']} ã«ä¿å­˜ä¸­..."):
                saved_files = []
                for genre, xml_content in xml_outputs_by_genre.items():
                    safe_genre_name = sanitize_filename(genre)
                    output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
                    output_file_path.write_text(xml_content, encoding="utf-8")
                    saved_files.append(output_file_path.name)
            
            files_table = Table(show_header=False, box=None)
            files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
            for file_name in saved_files:
                files_table.add_row(f"âœ“ {file_name}")
            
                console.print(Panel(files_table, title="[bold green]ğŸ’¾ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«[/bold green]", border_style="green"))
            
            # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            if export_alpaca:
                console.print("\n[bold blue]Alpacaå½¢å¼ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆä¸­...[/bold blue]")
                alpaca_file = dirs["base"] / "dataset_alpaca.json"
                alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                readme_file = dirs["base"] / "README.md"
                create_dataset_card(alpaca_data, readme_file, "Generated QA Dataset")
                
                # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
                if upload_hf:
                    if not hf_repo_name:
                        console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
                        console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
                    else:
                        console.print(f"\n[bold blue]Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...[/bold blue]")
                        success = upload_to_huggingface(
                            dataset_data=alpaca_data,
                            repo_name=hf_repo_name,
                            hf_token=hf_token if hf_token else None,
                            private=hf_private,
                            commit_message=f"Upload QA dataset with {len(alpaca_data)} entries",
                            readme_file=readme_file
                        )
                        if not success:
                            console.print("[bold red]Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ[/bold red]")
        else:
            console.print("\n--- ç”Ÿæˆã•ã‚ŒãŸQ&Aãƒšã‚¢ (Genreåˆ¥XML) ---")
            for genre, xml_content in xml_outputs_by_genre.items():
                console.print(f"\n[bold yellow]## Genre: {genre} ##[/bold yellow]")
                console.print(xml_content, overflow="fold")
    
    except Exception as e:
        import traceback
        error_details = f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}\n\nãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}"
        print_error_panel(error_details)
        raise typer.Exit(code=1)


@app.command()
def convert_to_alpaca(
    qa_dir: Annotated[Path, typer.Argument(
        exists=True, dir_okay=True, readable=True,
        help="XMLãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹qaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
    )],
    output_file: Annotated[Path, typer.Option(
        "--output-file", "-o", file_okay=True, dir_okay=False,
        help="å‡ºåŠ›ã™ã‚‹Alpacaå½¢å¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚"
    )] = None,
    upload_hf: Annotated[bool, typer.Option(
        "--upload-hf", "-u",
        help="ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
    )] = False,
    hf_repo_name: Annotated[str, typer.Option(
        "--hf-repo-name", "-r",
        help="Hugging Face Hubã®ãƒªãƒã‚¸ãƒˆãƒªåï¼ˆä¾‹: username/dataset-nameï¼‰"
    )] = "",
    hf_token: Annotated[str, typer.Option(
        "--hf-token", "-t",
        help="Hugging Face APIãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç’°å¢ƒå¤‰æ•°HUGGINGFACE_TOKENã‹ã‚‰ã‚‚å–å¾—å¯èƒ½ï¼‰"
    )] = "",
    hf_private: Annotated[bool, typer.Option(
        "--hf-private",
        help="Hugging Faceãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«ã—ã¾ã™ã€‚"
    )] = False,
):
    """æ—¢å­˜ã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã®JSONã«å¤‰æ›ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
    
    print_logo()
    
    conversion_table = Table(show_header=False, box=None)
    conversion_table.add_column("é …ç›®", style="bold cyan")
    conversion_table.add_column("å€¤", style="white")
    conversion_table.add_row("ğŸ“ å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª", str(qa_dir))
    conversion_table.add_row("ğŸ’¾ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«", str(output_file) if output_file else "è‡ªå‹•")
    if upload_hf:
        conversion_table.add_row("ğŸ¤— HFãƒªãƒã‚¸ãƒˆãƒª", hf_repo_name or "æœªæŒ‡å®š")
    
    console.print(Panel(conversion_table, title="[bold blue]ğŸ”„ Alpacaå½¢å¼å¤‰æ›[/bold blue]", border_style="blue"))
    
    try:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¨­å®š
        if output_file is None:
            output_file = qa_dir.parent / "dataset_alpaca.json"
        
        with console.status(f"ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’Alpacaå½¢å¼ã«å¤‰æ›ä¸­..."):
            alpaca_data = convert_all_xml_to_alpaca(qa_dir, output_file)
        
        if not alpaca_data:
            print_error_panel("å¤‰æ›ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            raise typer.Exit(code=1)
        
        with console.status("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆä¸­..."):
            readme_file = output_file.parent / "README.md"
            create_dataset_card(alpaca_data, readme_file, "Converted QA Dataset")
        
        # Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if upload_hf:
            if not hf_repo_name:
                print_error_panel("--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼\nä¾‹: --hf-repo-name username/my-qa-dataset")
                raise typer.Exit(code=1)
            
            with console.status(f"ğŸ¤— Hugging Face Hubã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                success = upload_to_huggingface(
                    dataset_data=alpaca_data,
                    repo_name=hf_repo_name,
                    hf_token=hf_token if hf_token else None,
                    private=hf_private,
                    commit_message=f"Upload converted QA dataset with {len(alpaca_data)} entries",
                    readme_file=readme_file
                )
            
            if not success:
                print_error_panel("Hugging Faceã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                raise typer.Exit(code=1)
        
        details = [
            f"{len(alpaca_data)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å¤‰æ›",
            f"å‡ºåŠ›å…ˆ: {output_file}",
            f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰: {readme_file}"
        ]
        if upload_hf and hf_repo_name:
            details.append(f"Hugging Face: {hf_repo_name}")
        
        print_success_summary("Alpacaå½¢å¼ã¸ã®å¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
        
    except Exception as e:
        print_error_panel(f"å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise typer.Exit(code=1)


@app.command()
def aggregate_logs(
    output_dir: Annotated[Path, typer.Argument(
        exists=True, dir_okay=True, readable=True,
        help="logsãƒ•ã‚©ãƒ«ãƒ€ãŒå«ã¾ã‚Œã‚‹å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã€‚"
    )]
):
    """logsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãXMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ã—ã¦qaãƒ•ã‚©ãƒ«ãƒ€ã®XMLã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
    print_logo()
    
    try:
        logs_dir = output_dir / "logs"
        qa_dir = output_dir / "qa"
        
        if not logs_dir.exists():
            print_error_panel(f"logsãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {logs_dir}")
            raise typer.Exit(code=1)
        
        aggregation_table = Table(show_header=False, box=None)
        aggregation_table.add_column("é …ç›®", style="bold cyan")
        aggregation_table.add_column("ãƒ‘ã‚¹", style="white")
        aggregation_table.add_row("ğŸ“ logsãƒ•ã‚©ãƒ«ãƒ€", str(logs_dir))
        aggregation_table.add_row("ğŸ¯ å‡ºåŠ›å…ˆ", str(qa_dir))
        
        console.print(Panel(aggregation_table, title="[bold blue]ğŸ“„ ãƒ­ã‚°é›†ç´„[/bold blue]", border_style="blue"))
        
        with console.status("ğŸ”„ XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é›†ç´„ä¸­..."):
            from easy_dataset_cli.core import aggregate_logs_xml_to_qa
            aggregate_logs_xml_to_qa(logs_dir, qa_dir)
        
        print_success_summary("ãƒ­ã‚°é›†ç´„ãŒå®Œäº†ã—ã¾ã—ãŸï¼", [f"å‡ºåŠ›å…ˆ: {qa_dir}"])
        
    except Exception as e:
        print_error_panel(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise typer.Exit(code=1)


@app.command(name="help", hidden=True)
def show_help():
    """ãƒ˜ãƒ«ãƒ—ã‚’ç¾ã—ãè¡¨ç¤º"""
    print_logo()
    console.print(app.get_help(typer.Context(app)))


def _batch_process_files(text_files, ga_file, ga_base_dir, output_dir, model, chunk_size, chunk_overlap,
                        num_qa_pairs, use_fulltext, use_thinking, append_mode,
                        export_alpaca, upload_hf, hf_repo_name, hf_token, hf_private):
    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹å†…éƒ¨é–¢æ•°ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼‰"""
    
    # GAãƒšã‚¢ã®è§£æã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«è¡Œã†ï¼ˆga_base_dirãƒ¢ãƒ¼ãƒ‰ã®å ´åˆï¼‰
    ga_pairs = None
    if ga_file:
        with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
            ga_pairs = parse_ga_file(ga_file)

        if not ga_pairs:
            print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            raise typer.Exit(code=1)

        console.print(f"\n[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")

    # ãƒ¢ãƒ¼ãƒ‰è­¦å‘Šã‚’è¡¨ç¤º
    warnings = []
    if use_fulltext:
        warnings.append("ğŸ“‹ å…¨æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰")
    if use_thinking:
        warnings.append("ğŸ¤” æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰")
    
    if warnings:
        warning_panel = Panel(
            "\n".join(warnings) + "\n\nâš ï¸ å‡¦ç†æ™‚é–“ã¨APIã‚³ã‚¹ãƒˆãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
            title="[bold yellow]âš ï¸ ãƒ¢ãƒ¼ãƒ‰è­¦å‘Š[/bold yellow]",
            border_style="yellow"
        )
        console.print(warning_panel)

    total_files = len(text_files)
    successful_files = []
    total_qa_pairs_generated = 0
    
    with Progress(console=console) as progress:
        main_task = progress.add_task("[green]ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...", total=total_files)
        
        for file_idx, text_file in enumerate(text_files):
            console.print(f"\n[bold cyan]å‡¦ç†ä¸­: {text_file.name}[/bold cyan]")
            
            try:
                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                file_output_dir = output_dir / text_file.stem
                dirs = create_output_directories(file_output_dir)
                console.print(f"[dim]âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {file_output_dir}[/dim]")
                
                text = text_file.read_text(encoding="utf-8")
                console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—[/dim]")
                
                # GAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ±ºå®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
                current_ga_path = None
                if ga_file:
                    # å¾“æ¥é€šã‚Šã€æŒ‡å®šã•ã‚ŒãŸå˜ä¸€ã®GAãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                    current_ga_path = ga_file
                    console.print(f"[dim]âœ“ ä½¿ç”¨ã™ã‚‹GAå®šç¾©: {current_ga_path}[/dim]")
                elif ga_base_dir:
                    # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¯¾å¿œã™ã‚‹GAãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’çµ„ã¿ç«‹ã¦ã‚‹
                    file_stem = text_file.stem
                    inferred_ga_path = ga_base_dir / file_stem / "ga" / "ga_definitions.xml"
                    
                    if inferred_ga_path.exists():
                        current_ga_path = inferred_ga_path
                        console.print(f"[dim]âœ“ GAå®šç¾©ã‚’è‡ªå‹•æ¤œå‡º: {current_ga_path}[/dim]")
                    else:
                        console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã«å¯¾å¿œã™ã‚‹GAå®šç¾©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚[/yellow]")
                        console.print(f"[dim]æ¤œç´¢ãƒ‘ã‚¹: {inferred_ga_path}[/dim]")
                        continue  # æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸
                
                # GAãƒšã‚¢ã‚’è§£æ
                with console.status("ğŸ” GAãƒšã‚¢ã‚’è§£æä¸­..."):
                    current_ga_pairs = parse_ga_file(current_ga_path)
                
                if not current_ga_pairs:
                    console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã®GAå®šç¾©ã‹ã‚‰æœ‰åŠ¹ãªGAãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚[/yellow]")
                    continue
                
                console.print(f"[green]âœ“[/green] {len(current_ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç™ºè¦‹")
                
                with console.status(f"âœ‚ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ä¸­... ({text_file.name})"):
                    chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                console.print(f"[green]âœ“[/green] {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ")
                
                all_qa_pairs_with_ga = []
                total_tasks_for_file = len(chunks) * len(current_ga_pairs)
                file_task = progress.add_task(f"[blue]{text_file.name}", total=total_tasks_for_file)

                for chunk in chunks:
                    for ga_pair in current_ga_pairs:
                        if use_thinking:
                            qa_pairs = generate_qa_for_chunk_with_ga_and_thinking(
                                chunk=chunk,
                                full_text=text if use_fulltext else "",
                                model=model,
                                ga_pair=ga_pair,
                                logs_dir=dirs["logs"],
                                num_qa_pairs=num_qa_pairs
                            )
                        elif use_fulltext:
                            qa_pairs = generate_qa_for_chunk_with_ga_and_fulltext(
                                chunk=chunk,
                                full_text=text,
                                model=model,
                                ga_pair=ga_pair,
                                logs_dir=dirs["logs"],
                                num_qa_pairs=num_qa_pairs
                            )
                        else:
                            qa_pairs = generate_qa_for_chunk_with_ga(
                                chunk, model=model, ga_pair=ga_pair,
                                logs_dir=dirs["logs"],
                                num_qa_pairs=num_qa_pairs
                            )

                        for pair in qa_pairs:
                            qa_entry = {
                                "genre": ga_pair['genre']['title'],
                                "audience": ga_pair['audience']['title'],
                                "question": pair['question'],
                                "answer": pair['answer']
                            }
                            all_qa_pairs_with_ga.append(qa_entry)

                        progress.update(
                            file_task, advance=1,
                            description=f"Genre: {ga_pair['genre']['title']}"
                        )
                
                progress.remove_task(file_task)
                
                # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®Q&Aãƒšã‚¢ã‚’XMLã«å¤‰æ›ã—ã¦ä¿å­˜
                xml_outputs_by_genre = convert_to_xml_by_genre(all_qa_pairs_with_ga, dirs["qa"], append_mode)
                
                saved_files = []
                for genre, xml_content in xml_outputs_by_genre.items():
                    safe_genre_name = sanitize_filename(genre)
                    output_file_path = dirs["qa"] / f"{safe_genre_name}.xml"
                    output_file_path.write_text(xml_content, encoding="utf-8")
                    saved_files.append(output_file_path.name)
                
                # ã‚¢ãƒ«ãƒ‘ã‚«å½¢å¼ã§ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å€‹åˆ¥ï¼‰
                if export_alpaca:
                    alpaca_file = dirs["base"] / "dataset_alpaca.json"
                    alpaca_data = convert_all_xml_to_alpaca(dirs["qa"], alpaca_file)
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                    readme_file = dirs["base"] / "README.md"
                    create_dataset_card(alpaca_data, readme_file, f"Generated QA Dataset from {text_file.name}")
                
                successful_files.append((text_file.name, file_output_dir, len(all_qa_pairs_with_ga), saved_files))
                total_qa_pairs_generated += len(all_qa_pairs_with_ga)
                console.print(f"[green]âœ“[/green] {len(all_qa_pairs_with_ga)}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ")
                
            except Exception as e:
                console.print(f"[red]ã‚¨ãƒ©ãƒ¼: {text_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
                continue
            
            progress.update(
                main_task, advance=1,
                description=f"å®Œäº†: {text_file.name}"
            )

    if not successful_files:
        print_error_panel("æœ‰åŠ¹ãªQ&Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        raise typer.Exit(code=1)

    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
    details = [
        f"{total_qa_pairs_generated}å€‹ã®Q&Aãƒšã‚¢ã‚’ç”Ÿæˆ",
        f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(successful_files)}/{total_files}å€‹",
        f"å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"
    ]
    
    # å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    files_table = Table(show_header=True, box=None)
    files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
    files_table.add_column("ãƒ•ã‚©ãƒ«ãƒ€", style="white")  
    files_table.add_column("Q&Aãƒšã‚¢æ•°", style="green")
    
    for file_name, output_path, qa_count, _ in successful_files:
        files_table.add_row(file_name, str(output_path), str(qa_count))
    
    console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†çµæœ[/bold green]", border_style="green"))
    
    print_success_summary("ãƒãƒƒãƒQ&Aç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
    
    # Hugging Face Hubã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆæœ€åˆã®æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå€‹åˆ¥æŒ‡å®šï¼‰
    if upload_hf and export_alpaca:
        if not hf_repo_name:
            console.print("[bold red]--hf-repo-nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼[/bold red]")
            console.print("[yellow]ä¾‹: --hf-repo-name username/my-qa-dataset[/yellow]")
        else:
            console.print(f"\n[bold blue]Hugging Face Hub ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«ã¤ã„ã¦[/bold blue]")
            console.print("[yellow]æ³¨æ„: ç¾åœ¨ã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ãŒå€‹åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€")
            console.print("å€‹åˆ¥ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€çµ±åˆã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚[/yellow]")


def _batch_create_ga_files(text_files, output_dir, model, num_ga_pairs):
    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GAãƒšã‚¢ã‚’ãƒãƒƒãƒç”Ÿæˆã™ã‚‹å†…éƒ¨é–¢æ•°ï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼‰"""
    
    total_files = len(text_files)
    successful_files = []
    
    with Progress(console=console) as progress:
        main_task = progress.add_task("[green]GAãƒšã‚¢ã‚’ç”Ÿæˆä¸­...", total=total_files)
        
        for file_idx, text_file in enumerate(text_files):
            console.print(f"\n[bold cyan]å‡¦ç†ä¸­: {text_file.name}[/bold cyan]")
            
            try:
                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                file_output_dir = output_dir / text_file.stem
                dirs = create_output_directories(file_output_dir)
                console.print(f"[dim]âœ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {file_output_dir}[/dim]")
                
                text = text_file.read_text(encoding="utf-8")
                console.print(f"[dim]âœ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text):,} æ–‡å­—[/dim]")

                with console.status(f"[bold green]ğŸ¤– LLMã«GAãƒšã‚¢ã®ææ¡ˆã‚’ä¾é ¼ä¸­... ({text_file.name})[/bold green]"):
                    xml_content = generate_ga_definitions(text, model=model, num_ga_pairs=num_ga_pairs)

                # LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                raw_file_path = dirs["logs"] / "raw.md"
                raw_file_path.write_text(xml_content, encoding="utf-8")
                console.print(f"[green]âœ“[/green] LLMã®rawãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¿å­˜: [cyan]{raw_file_path}[/cyan]")

                with console.status(f"[bold green]ğŸ” XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æä¸­... ({text_file.name})[/bold green]"):
                    # XMLã‹ã‚‰GAãƒšã‚¢ã‚’è§£æ
                    ga_pairs = parse_ga_definitions_from_xml(xml_content)
                
                if not ga_pairs:
                    console.print(f"[yellow]è­¦å‘Š: {text_file.name} ã‹ã‚‰ã¯æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ[/yellow]")
                    continue

                # å…ƒã®XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªXMLã®ã¿ï¼‰
                xml_file_path = dirs["ga"] / "ga_definitions.xml"
                xml_start = xml_content.find("<GADefinitions>")
                xml_end = xml_content.rfind("</GADefinitions>")
                if xml_start != -1 and xml_end != -1:
                    clean_xml = xml_content[xml_start: xml_end + len("</GADefinitions>")]
                    xml_file_path.write_text(clean_xml, encoding="utf-8")
                    console.print(f"[green]âœ“[/green] GAå®šç¾©XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: [cyan]{xml_file_path}[/cyan]")

                # Genreã”ã¨ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’gaãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                save_ga_definitions_by_genre(ga_pairs, dirs["ga"])
                
                successful_files.append((text_file.name, file_output_dir, len(ga_pairs)))
                console.print(f"[green]âœ“[/green] {len(ga_pairs)}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                
            except Exception as e:
                console.print(f"[red]ã‚¨ãƒ©ãƒ¼: {text_file.name} ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}[/red]")
                continue
            
            progress.update(
                main_task, advance=1,
                description=f"å®Œäº†: {text_file.name}"
            )

    if not successful_files:
        print_error_panel("æœ‰åŠ¹ãªGAãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\nç”Ÿæˆã•ã‚ŒãŸXMLã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        raise typer.Exit(code=1)

    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¾ã—ãè¡¨ç¤º
    total_ga_pairs = sum(count for _, _, count in successful_files)
    details = [
        f"{total_ga_pairs}å€‹ã®GAãƒšã‚¢ã‚’ç”Ÿæˆ",
        f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«: {len(successful_files)}/{total_files}å€‹",
        f"å„ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ"
    ]
    
    # å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    files_table = Table(show_header=True, box=None)
    files_table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«", style="cyan")
    files_table.add_column("ãƒ•ã‚©ãƒ«ãƒ€", style="white")
    files_table.add_column("GAãƒšã‚¢æ•°", style="green")
    
    for file_name, output_path, ga_count in successful_files:
        files_table.add_row(file_name, str(output_path), str(ga_count))
    
    console.print(Panel(files_table, title="[bold green]ğŸ“„ å‡¦ç†çµæœ[/bold green]", border_style="green"))
    
    print_success_summary("ãƒãƒƒãƒGAãƒšã‚¢ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼", details)
    
    next_steps_panel = Panel(
        "ğŸ” ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼\n"
        "âœï¸ å¿…è¦ã«å¿œã˜ã¦ç·¨é›†\n"
        "ğŸš€ `generate` ã‚³ãƒãƒ³ãƒ‰ã§Q&Aç”Ÿæˆã¸",
        title="[bold yellow]ğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—[/bold yellow]",
        border_style="yellow"
    )
    console.print(next_steps_panel)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - ãƒ˜ãƒ«ãƒ—æ™‚ã«ãƒ­ã‚´ã‚’è¡¨ç¤º"""
    import sys
    if len(sys.argv) == 1 or (len(sys.argv) == 2 and sys.argv[1] in ["-h", "--help"]):
        print_logo()
    app()


if __name__ == "__main__":
    main()
